\section{Conclusions}

The most crucial thing about working with a dataset such as this one is getting the \textit{\textbf{preprocessing}} right. We strongly believe that our results were this good due to the extensive preprocessing that we performed on the dataset provided to us. Removing stop-words, lemmatizing words, negation handling and general text clean up really helped process the data better in order for it to be trained and tested upon. A cool feature we managed to incorporate during this phase was Emoji Tagging. Looking at the raw reviews provided, we realized that an important feature that could be exploited from the reviews were the presence of emojis (textual forms of sad/happy faces). We leveraged these emojis being present in the reviews to help better our prediction.

We did our best to reduce the number of features using a couple of feature selection techniques such as K-Best (with the Chi-square Test) and LSA. K-Best was found to work well and was computationally faster than LSA, so based on performance we went ahead with that but it would be interesting to see if other Feature selection techniques would help to improve the performance of the classifiers. For example, SVD and PCA could be interesting to compare to one another.

Word vectors prove to be efficient in RNNs, however, larger the data,  more the computations it requires.

Additionally, working with dichotomous, highly overlapping data, doesn't lend itself to any one particular classifier. This is intuitive from the 2-D plots of the data that we saw in Section-\ref{sec:dataexploration}. If you encounter datasets such as these, it is highly likely that ensembles will work best; given the robust preprocessing of data done prior to combining the models. It is important to note that the weights, if using the Average Weights technique, should be assigned to the models based on their individual performances. Moreover, trying to combine the models that worked individually well would most likely result in the best ensembles.
