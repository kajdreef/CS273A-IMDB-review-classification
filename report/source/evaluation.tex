\section{Project Validation}

\subsection{Model Evaluation Metrics}
In Table~\ref{tab:classifier-comp}, we show our optimized classifiers using 5 metrics. The metrics are as follows:
\begin{enumerate}
    \item \textbf{AUC}: Area under Curve, gives an idea how well a classifier is performing; 0.5 is the same as random guessing. The closer the AUC to 1.0, the better the classifier.
    \item \textbf{Mean AUC 10-fold CV}: Similar to AUC, but gives an idea if our classifier generalizes independent of the data set.
    \item \textbf{F1-score}: Measurement of the accuracy of the classifier.
    \item \textbf{Precision}: Ability of the classifier to correctly predict a positive item as positive.
    \item \textbf{Recall}: Ability of the classifier to locate all positive items.
\end{enumerate}


\begin{table*}[!ht]
    \centering
    \caption{Results of different classifiers.\label{tab:classifier-comp}}
    \begin{tabular}{l | S[table-format=1.3] | S[table-format=1.3] | S[table-format=1.3] | S[table-format=1.3] | S[table-format=1.3]}
    \hline
    \bf{Classifier} & \bf{AUC} & \bf{Mean AUC 10-fold CV} & \bf{F1-score} & \bf{Precision} & \bf{Recall} \\
    \hline
    Decision Trees & 0.76200 & 0.1889 & 0.6954 & 0.7170 & 0.6750 \\
    Random Forest & 0.84996 & 0.84788 & 0.8473776294909874 & 0.8622174381054898 & 0.83304 \\
    Logistic Regression & 0.8852800000000001 & 0.8947200000000001 & 0.8856641683941956 & 0.8827082008900191 & 0.88864 \\
    Linear SVM SGD & 0.8715999999999999 & 0.87144 & 0.8733527972855678 & 0.8615911567803207 & 0.88544 \\
    Logistic SGD & 0.85904 & 0.8581199999999999 & 0.8609971599873777 & 0.8492063492063492 & 0.87312 \\
    kNN & 0.57428 & 0.5864800000000001 & 0.6197162968521099 & 0.5599535093949765 & 0.69376 \\
    MLP & 0.8210000000000001 & 0.87468 & 0.8159874994859987 & 0.8394957272188849 & 0.79376 \\
    \hline
    \end{tabular}
\end{table*}

\begin{table*}[!ht]
    \centering
    \caption{Results of certain Weighted Average Ensembles created using our previously modeled Classifiers\label{tab:evaluation-ensembles}}
    \begin{tabular}{ l| S[table-format=1.4] |S[table-format=1.4] |S[table-format=1.4] | S[table-format=1.4] |S[table-format=1.4] |S[table-format=1.4] }
    \hline
        \bf{Classifiers in the Ensemble} & {\bf{Prediction Weight}} & {\bf{AUC}} & {\bf{10-Fold Cross Validation}} & {\bf{F1-measure}} & {\bf{Precision}} & {\bf{Recall}} \\
    \hline
        Random Forest & 0.65 & 0.9000 & 0.8648 & 0.8455 & 0.8596 & 0.8320 \\ 
        Logistic Regression & 0.35 &  &  &  &  & \\
    \hline
        Random Forest & 0.35 & 0.9054 & 0.8798 & 0.8856 & 0.8826 & 0.8886\\
        Logistic Regression & 0.65 &  &  &  &  & \\
    \hline
        Random Forest & 0.35 & 0.8844 & 0.8536 & 0.8619 & 0.8489 & 0.8752 \\
        Logistic SGD & 0.65 &  &  &  &  & \\
    \hline
        Random Forest & 0.35 & 0.8941 & 0.8646 & 0.8726 & 0.8549 & 0.8912 \\
        Linear SVM SGD & 0.65 &  &  &  &  & \\
    \hline
        Logistic Regression & 0.65 & 0.9007 & 0.8872 & 0.8856 & 0.8826 & 0.8886 \\
        Linear SVM SGD & 0.35 &  &  &  &  & \\
    \hline
        Logistic Regression & 0.55 & 0.9014 & 0.8848 & 0.8856 & 0.8826 & 0.8886 \\
        Linear SVM SGD & 0.45 &  &  &  &  & \\
    \hline
        Random Forest & 0.20 &  &  &  &  & \\ 
        Logistic Regression & 0.45 & 0.9110 & 0.8781 & 0.8790 & 0.8738 & 0.8842 \\
        Linear SVM SGD & 0.35 &  &  &  &  & \\
    \hline
        Random Forest & 0.23 &  &  &  &  & \\ 
        Logistic Regression & 0.40 & 0.9113 & 0.8758 & 0.8782 & 0.8754 & 0.8811 \\
        Linear SVM SGD & 0.37 &  &  &  &  & \\
    \hline
    \end{tabular}
\end{table*}

\subsection{Understanding the Results}

\begin{itemize}
    \item
    We have a binary/dichotomous Y variable, i.e., our prediction involves one of 2 possible results - a positive review or a negative review. For this reason, we see that Logistic Regression worked best (as an standalone classifier) as compared to a more robust Ensembling method like Random Forest.
    \item 
    After Logistic Regression we decided to implement decision trees to see how well it would perform. The Decsion Tree classifier was optimized to an AUC of 0.76. The crucial parameter that was adjusted was "min\_samples\_split" - which was varied between 50 and 10,000 - and the optimum one was found to be 600. We also restricted the maximum number of features to be the square root of the total number of features set.
    We later decided to explore an ensembling method to bolster the performance of the Decision Trees classifier, by implementing the Random Forest classifier. This is because RF has lesser overfitting than the regular Decision Trees.
    \item
    Random Forest achieved an AUC result of 0.85, which is pretty significant bump over the Decision Trees' classifier's performance. But, to our surprise Logistic Regression still beat Random Forest in its performance. This is due to the the densely populated, overlapped dataset we're working with. The Random forest, although performed decently with 0.85 AUC, was not able to partition all the data precisely since there was much overlap in the data points. We found the optimal \textit{number of trees} (n\_estimators) for the Random Forest to be 100.
    \item
    Linear SVM SGD as well as Logistic SGD performed quite well too. Even better than Random Forests , but not by much. This is because of the same reason as before. Random Forests would not work with such a densely populated dataset.
    \item As seen in Table-\ref{tab:evaluation-ensembles}, Ensembles worked the best for the given data set. The best AUC result was received with the ensemble of Random Forest, Logistic Regression and Linear SVM SGD, which individually were the best performers too. The AUC for the same is 0.9113. We gave preference to the result of Logistic Regression, followed by Linear SVM SGD, followed by Random Forests since the AUC and Cross-validation scores for each of them were in descending order. Even with a simple Weighted Average technique, we were able to give significant preference to the decisions made by each of the classifiers and combine them to get a better result. Even for the ensembles that were pairs, we ensured that the priority weight was given to the model with the better, individual AUC and Cross-validation score.
    \item It is also interesting to note that pairs of the classifiers didn't do as well as the 3 classifiers combined. But again, not by much.
\end{itemize}