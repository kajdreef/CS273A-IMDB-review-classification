{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from main import load_data, Extractor, remove_stop_words, negation_handling, clean_text, lemmatizing\n",
    "from preprocessing import emoji_tagging\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from util import curry\n",
    "from pprint import pprint\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and do some preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data...\n",
      "Done loading data!\n",
      "\n",
      "time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "Xtr_text, Ytr, Xte_text, Yte = load_data('../aclImdb/train/', '../aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6min 5s\n"
     ]
    }
   ],
   "source": [
    "extractor1 = Extractor(Xtr_text, Xte_text)\\\n",
    "        .bind(curry(emoji_tagging))\\\n",
    "        .bind(curry(remove_stop_words))\\\n",
    "        .bind(curry(lemmatizing))\\\n",
    "        .bind(curry(negation_handling))\\\n",
    "        .bind(curry(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "extractor2 = Extractor(Xtr_text, Xte_text)\\\n",
    "        .bind(curry(remove_stop_words))\\\n",
    "        .bind(curry(lemmatizing))\\\n",
    "        .bind(curry(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "extractor3 = Extractor(Xtr_text, Xte_text)\\\n",
    "        .bind(curry(lemmatizing))\\\n",
    "        .bind(curry(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 39.5 s\n"
     ]
    }
   ],
   "source": [
    "extractor4 = Extractor(Xtr_text, Xte_text)\\\n",
    "        .bind(curry(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.12 ms\n"
     ]
    }
   ],
   "source": [
    "Xtr1, Xte1 = extractor1.get_features()\n",
    "Xtr2, Xte2 = extractor2.get_features()\n",
    "Xtr3, Xte3 = extractor3.get_features()\n",
    "Xtr4, Xte4 = extractor4.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f1a5bdf97aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXte1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "print(Xtr1.shape)\n",
    "print(Xte1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Training Dataset up in positive and negative sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_np = np.array(Xtr1)\n",
    "Xte_np = np.array(Xte1)\n",
    "\n",
    "Xtr_pos = Xtr_np[Ytr>0]\n",
    "Xtr_neg = Xtr_np[Ytr<1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_review_pos = \" \".join(Xtr_pos)\n",
    "super_review_neg = \" \".join(Xtr_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos = word_tokenize(super_review_pos)\n",
    "tokens_neg = word_tokenize(super_review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequency_dist_pos = nltk.FreqDist(tokens_pos)\n",
    "frequency_dist_neg = nltk.FreqDist(tokens_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sorted(frequency_dist_pos.items(), key=lambda x:-x[1])[:50]\n",
    "neg = sorted(frequency_dist_neg.items(), key=lambda x:-x[1])[:50]\n",
    "print(pos)\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a wordcloud based on the top 50 words in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(50,50))\n",
    "\n",
    "wordcloud_pos = WordCloud().generate_from_frequencies(frequency_dist_pos)\n",
    "wordcloud_neg = WordCloud().generate_from_frequencies(frequency_dist_pos)\n",
    "\n",
    "plt.imshow(wordcloud_pos)\n",
    "plt.title(\"Positive\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"positive_wordcloud.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(wordcloud_neg)\n",
    "plt.title(\"Negative\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"negative_wordcloud.png\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection: Kbest based on Chi test and ngram range performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(Xtr, Ytr, Xte, Yte, ngram_r=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_r)\n",
    "\n",
    "    vectorizer.fit(Xtr, Ytr)\n",
    "    XtrS = vectorizer.transform(Xtr)\n",
    "    XteS = vectorizer.transform(Xte)\n",
    "    print(\"Number of Features: \", len(vectorizer.get_feature_names()))\n",
    "    \n",
    "    return XtrS, XteS\n",
    "\n",
    "def k_Best(Xtr, Ytr, Xte, Yte, k_best):\n",
    "    Select = SelectKBest(chi2, k=k_best).fit(Xtr, Ytr)\n",
    "    XtrS = Select.transform(Xtr)\n",
    "    XteS = Select.transform(Xte)\n",
    "\n",
    "    print(\"Number of features after select KBest\", k_best)\n",
    "    \n",
    "    return XtrS, XteS\n",
    "\n",
    "def plot_scores(k_list, f1_scores, p_scores, r_scores, labels, save_to_file=False):\n",
    "    for f1, k in zip(f1_scores, k_list):\n",
    "        plt.semilogx(k, f1)\n",
    "        plt.title(\"F-1 score for best k features\")\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel(\"k\")\n",
    "    if save_to_file:\n",
    "        plt.savefig('f1-score.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for p, k in zip(p_scores, k_list):\n",
    "        plt.semilogx(k, p)\n",
    "    plt.title(\"Precision for best k features\")\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel(\"k\")\n",
    "    if save_to_file:\n",
    "        plt.savefig('precision-score.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "    for r, k in zip(r_scores, k_list):\n",
    "        plt.semilogx(k, r)\n",
    "    plt.title(\"Recall for best k features\")\n",
    "    plt.legend(labels)\n",
    "    plt.xlabel(\"k\")\n",
    "    if save_to_file:\n",
    "        plt.savefig('recall-score.png', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_11_1, Xte_11_1 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(1,1))\n",
    "Xtr_11_2, Xte_11_2 = vectorize(Xtr2, Ytr, Xte2, Yte, ngram_r=(1,1))\n",
    "Xtr_11_3, Xte_11_3 = vectorize(Xtr3, Ytr, Xte4, Yte, ngram_r=(1,1))\n",
    "Xtr_11_4, Xte_11_4 = vectorize(Xtr4, Ytr, Xte4, Yte, ngram_r=(1,1))\n",
    "\n",
    "Xtr_set = [Xtr_11_1, Xtr_11_2, Xtr_11_3, Xtr_11_4]\n",
    "Xte_set = [Xte_11_1, Xte_11_2, Xte_11_3, Xte_11_4]\n",
    "\n",
    "k_list = [\n",
    "    [10, 100, 1000, 10000, 50000, 88391],\n",
    "    [10, 100, 1000, 10000, 50000, 69950],\n",
    "    [10, 100, 1000, 10000, 50000, 72198],\n",
    "    [10, 100, 1000, 10000, 50000, 73986]\n",
    "]\n",
    "\n",
    "labels=[\"All preprocessing\", \"wo negation/emoji\", \"wo negation/emoji/stopword_removing\", \"Only clean text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtr_11, Xte_11 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(1,1))\n",
    "# Xtr_22, Xte_22 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(2,2))\n",
    "# Xtr_33, Xte_33 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(3,3))\n",
    "# Xtr_12, Xte_12 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(1,2))\n",
    "# Xtr_13, Xte_13 = vectorize(Xtr1, Ytr, Xte1, Yte, ngram_r=(1,3))\n",
    "\n",
    "# Xtr_set = [Xtr_11, Xtr_22, Xtr_33, Xtr_12, Xtr_13]\n",
    "# Xte_set = [Xte_11, Xte_22, Xte_33, Xte_12, Xte_13]\n",
    "\n",
    "# k_list = [\n",
    "#     [10, 100, 1000, 10000, 50000, 88391],\n",
    "#     [10, 100, 1000, 10000, 100000, 1000000, 1790783],\n",
    "#     [10, 100, 1000, 10000, 100000, 1000000, 2842984],\n",
    "#     [10, 100, 1000, 10000, 100000, 1000000, 1879174],\n",
    "#     [10, 100, 1000, 10000, 100000, 1000000, 4722158]\n",
    "# ]\n",
    "\n",
    "# labels=[\"ngram=(1,1)\", \"ngram=(2,2)\", \"ngram=(3,3)\", \"ngram=(1,2)\", \"ngram=(1,3)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f1_scores_all = []\n",
    "p_scores_all = []\n",
    "r_scores_all  = []\n",
    "\n",
    "for (Xtrain, Xtest), k_best in zip(zip(Xtr_set, Xte_set), k_list):\n",
    "    f1_scores = []\n",
    "    p_scores = []\n",
    "    r_scores  = []\n",
    "    \n",
    "    for k in k_best:\n",
    "        Xtr_vec, Xte_vec = k_Best(Xtrain, Ytr, Xtest, Yte, k)\n",
    "        classifier = LogisticRegression(solver='lbfgs')\n",
    "        classifier.fit(Xtr_vec, Ytr)\n",
    "        Yte_hat = classifier.predict(Xte_vec)\n",
    "\n",
    "        f1_scores.append(f1_score(Yte, Yte_hat))\n",
    "        p_scores.append(precision_score(Yte, Yte_hat))\n",
    "        r_scores.append(recall_score(Yte, Yte_hat))\n",
    "\n",
    "    f1_scores_all.append(f1_scores)\n",
    "    p_scores_all.append(p_scores)\n",
    "    r_scores_all.append(r_scores)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(k_list, f1_scores_all, p_scores_all, r_scores_all, labels, save_to_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
